# CryoET Protein Identification üß¨

This project focuses on identifying and segmenting proteins in cryo-electron tomography (cryo-ET) data using deep learning. It leverages a 3D U-Net style architecture with ConvNeXt blocks to achieve accurate segmentation of proteins within cryo-ET volumes.

This is a modified copy in terms of code of the notebook of the 9th place kaggle competition solution with small steps towards understanding MLOps and how projects should be organised and managed.

The aim of the project is to train the model and then to create 'submissions.csv' file with test results in the format that was expected in the kaggle competition.

As there were some troubles with downloading the dataset from the kaggle using cli, the dataset was uploaded to a google drive via dvc remote.

## üöÄ Key Features

- **3D Segmentation:** Utilizes a 3D U-Net architecture for accurate segmentation of proteins in cryo-ET volumes.
- **ConvNeXt Blocks:** Employs ConvNeXt blocks for efficient feature extraction and representation learning.
- **PyTorch Lightning:** Leverages PyTorch Lightning for streamlined training, validation, and experiment management.
- **Data Augmentation:** Includes data augmentation techniques (rotation, flipping, intensity shifting) to improve model robustness.
- **Configuration Management:** Uses Hydra for flexible and organized configuration of training parameters.
- **Experiment Tracking:** Integrates with Weights & Biases (Wandb) for comprehensive experiment tracking and visualization.
- **Zarr Support:** Supports loading cryo-ET volumes and segmentation masks stored in Zarr format.
- **BCEDice Loss:** Combines Binary Cross-Entropy and Dice loss for effective segmentation training.

## üõ†Ô∏è Tech Stack

*   **Training Framework:** `PyTorch Lightning`
*   **Cryo-ET Data Format:** `Zarr`
*   **Experiment Tracking:** `Weights & Biases (Wandb)`
*   **Configuration Management:** `Hydra`
*   **Loss Function:** Custom `BCEDiceLoss`
*   **Optimizer:** `AdamW`
*   **Learning Rate Scheduler:** `CosineAnnealingLR`

## Data Formats and Storage

* Cryo-ET volumes and segmentation masks are stored in Zarr format.
* kaggle/input directory contains everything from the original kaggle dataset fro convinience as cli kaggle download timeouted this dataset
* inference results are a csv file with specified from the kaggle competition format, example can be found kaggle/input/sample_submission.csv
* generate_masks command creates masks (aka spheras on 3d tomograms) to be interpreted as protein particles from the input picks xy and radius aand are stored in working/overlay/
* model checpoints would be located by default in kaggle/working/output
* there is a convert_to_onnx command to create a model export in onnx format at kaggle/working/output/model.onnx by default


## üì¶ Getting Started

### Preparation

1.  **Clone the repository:**


2.  **Install poetry:**
    The recommended way is to install via pipx:
    ```bash
    pipx install poetry
    ```

3.  **Install the dependencies:**
    ```bash
    poetry install
    ```

4. **Pull files:**
    ```bash
    poetry run dvc pull
    ```

### Running

1.  **Configure configs:**
    Modify the `configs/train.yaml` file to suit your needs if necessary.
    Modify the `configs/infer.yaml` file to suit your needs if necessary.
    Adjust data paths, batch sizes, and other training parameters as necessary.

2.  **All commands are accesible through command.py:**

    generate_masks to generate masks from input data (if you would like to adjust radius scales for diffrent particles, default masks are already stored in dvc remote)

    vizualize_masks to gain better understanding of what's going on

    train, infer and convert_to_onnx

    ```bash
    poetry run python3 commands.py train
    ```


## üì∏ Screenshots

Sample wandb report can be found (and i hope publically accesd by anyone) here: https://api.wandb.ai/links/fetveta-org/vl5oed40

## üìù License

There is no license -- this is not a project from scratch, it's just a project for course in university in which i try to implement mlops should-does for code i borrowed from kaggle competition winner.

## üì¨ Contact

If for any reason ever you would feel like contacting me than you can try to reach me at [fetveta@gmail.com](mailto:fetveta@gmail.com).
No promise of responce though as i would be too embarassed of existence of this repo.

## üíñ Thanks

Thanks for whatever reason you scrolled through this obviously not partually generated by ai readme
Would appreciate some memes sent to my gmail above
