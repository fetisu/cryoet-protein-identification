# CryoET Protein Identification 🧬

This project focuses on identifying and segmenting proteins in cryo-electron tomography (cryo-ET) data using deep learning. It leverages a 3D U-Net style architecture with ConvNeXt blocks to achieve accurate segmentation of proteins within cryo-ET volumes.

This is a modified copy in terms of code of the notebook of the 9th place kaggle competition solution with small steps towards understanding MLOps and how projects should be organised and managed.

The aim of the project is to train the model and then to create 'submissions.csv' file with test results in the format that was expected in the kaggle competition.

As there were some troubles with downloading the dataset from the kaggle using cli, the dataset was uploaded to a google drive via dvc remote.

## 🚀 Key Features

- **3D Segmentation:** Utilizes a 3D U-Net architecture for accurate segmentation of proteins in cryo-ET volumes.
- **ConvNeXt Blocks:** Employs ConvNeXt blocks for efficient feature extraction and representation learning.
- **PyTorch Lightning:** Leverages PyTorch Lightning for streamlined training, validation, and experiment management.
- **Data Augmentation:** Includes data augmentation techniques (rotation, flipping, intensity shifting) to improve model robustness.
- **Configuration Management:** Uses Hydra for flexible and organized configuration of training parameters.
- **Experiment Tracking:** Integrates with Weights & Biases (Wandb) for comprehensive experiment tracking and visualization.
- **Zarr Support:** Supports loading cryo-ET volumes and segmentation masks stored in Zarr format.
- **BCEDice Loss:** Combines Binary Cross-Entropy and Dice loss for effective segmentation training.

## 🛠️ Tech Stack

*   **Training Framework:** `PyTorch Lightning`
*   **Cryo-ET Data Format:** `Zarr`
*   **Experiment Tracking:** `Weights & Biases (Wandb)`
*   **Configuration Management:** `Hydra`
*   **Loss Function:** Custom `BCEDiceLoss`
*   **Optimizer:** `AdamW`
*   **Learning Rate Scheduler:** `CosineAnnealingLR`

## 📦 Getting Started

### Preparation

1.  **Clone the repository:**


2.  **Install poetry:**
    The recommended way is to install via pipx:
    ```bash
    pipx install poetry
    ```

3.  **Install the dependencies:**
    ```bash
    poetry install
    ```

4. **Pull files:**
    ```bash
    poetry run dvc pull
    ```

### Running

1.  **Configure the training parameters:**
    Modify the `configs/train.yaml` file to suit your needs.  Adjust data paths, batch sizes, and other training parameters as necessary.

2.  **Start the training process:**

    ```bash
    poetry run python3 train.py
    ```

3. **Configure the inference parameters:**
    Modify the `configs/infer.yaml` file to suit your needs and run it!
    Note that it uses models from checkpoints.

3. **Inference:**
    ```bash
    poetry run python3 infer.py
    ```


## 📂 Project Structure

```
cryoet-protein-identification/
├── configs/
│   └── train.yaml          # Training configuration file
├── cryoet-protein-identification/
│   ├── dataset.py          # Dataset class for loading and preprocessing data
│   ├── eval_loss.py        # Loss functions and evaluation metrics
│   ├── infer.py            # Inference script (currently empty)
│   ├── rotate_flip.py      # Data augmentation functions
│   ├── threed_models.py    # 3D neural network architectures
│   ├── train.py            # Training script
│   └── utils.py            # Utility functions and constants
├── README.md             # This file
└── requirements.txt      # Project dependencies (example)
```

## 📸 Screenshots



## 📝 License

There is no license -- this is not a project from scratch, it's just a project for course in university in which i try to implement mlops should-does for code i borrowed from kaggle competition winner.

## 📬 Contact

If for any reason ever you would feel like contacting me than you can try to reach me at [fetveta@gmail.com](mailto:fetveta@gmail.com).
No promise of responce though as i would be too embarassed of existence of this repo.

## 💖 Thanks

Thanks for whatever reason you scrolled through this obviously not partually generated by ai readme
Would appreciate some memes sent to my gmail above
